{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Context**: explaining machine learning algorithms is a broad topic and many methods have already been introduced, such as the popular [LIME](https://arxiv.org/pdf/1602.04938.pdf). However, explaining outliers remains especially difficult. This is because most methods explain a decision in creating an neighborhood around the instance to explain. But creating a neighborhood around an outlier can be difficult; by definition, the outlier is an isolated instance and its closest neighbor can be far away.\n",
    "\n",
    "**Objective**: this notebook is an attempt to replicate the LIME method and adapt it in case of outliers.\n",
    "\n",
    "**Method**: the method we used is structured as such:\n",
    "\n",
    "1. Perform SMOTE to have a more balanced dataset\n",
    "\n",
    "2. Define a neighborhood according to class ratio\n",
    "\n",
    "3. Do a cross validation to find the best regularization strength\n",
    "\n",
    "    a) Compute weights to give less importance on the furthest point\n",
    "    \n",
    "    b) Perform logistic regression\n",
    "    \n",
    "    \n",
    "4. Explain using coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Â 1. SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[SMOTE](https://arxiv.org/pdf/1106.1813.pdf) is an algorithm allowing to oversample data that are imbalanced.\n",
    "\n",
    "SMOTE algorithm works as such:\n",
    "\n",
    "- loops on every outlier\n",
    "\n",
    "- defines a neighborhood for each of them\n",
    "\n",
    "- selects a random neighbor\n",
    "\n",
    "- loops on every attribute and adds a random-weighted distance to the random neighbor: \n",
    "\n",
    "```generated_sample[attr] = outlier[attr] + (neighbor[attr]-outlier[attr])*random(0,1)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performSMOTE(data, labels):\n",
    "    oversample = SMOTE(sampling_strategy=0.05) # we want at least 5% of minority class\n",
    "    data_aug, label_aug = oversample.fit_resample(data, labels)\n",
    "    label_aug.columns = ['Outlier_Inlier']\n",
    "    nb_generated_outliers = data_aug.shape[0] - data.shape[0]\n",
    "    print(\"number of generated outliers: {}\".format(nb_generated_outliers))\n",
    "    return data_aug, label_aug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define a neighborhood according to class ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform an efficient local classification, we need to have a relatively balanced neighoborhood. Thus, we want at least 10% of the neighborhood minor class.\n",
    "\n",
    "We start with 20 neighbors and increase this number until we have a balanced neighborhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectNeighborhood(data, labels, outlier):\n",
    "    ratio_outliers = 1\n",
    "    n_neighbors = 20\n",
    "    while (ratio_outliers < 0.1 or ratio_outliers > 0.9): # we want at least 10% of the neighbor minor class\n",
    "        n_neighbors = n_neighbors + 1\n",
    "        if (n_neighbors % 100) == 0:\n",
    "            print(\"trying with {} neighbors\".format(n_neighbors))\n",
    "        nbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm='ball_tree').fit(data)\n",
    "        distances, indices = nbrs.kneighbors(np.reshape(np.array(outlier),(1,-1)))\n",
    "        distances = distances.flatten()\n",
    "        indices = indices.flatten()\n",
    "        X = data.iloc[indices,:]\n",
    "        Y = labels.iloc[indices,:]\n",
    "        nb_outliers_neighborhood = Y[Y['Outlier_Inlier']=='Outlier'].shape[0]\n",
    "        nb_inliers_neighborhood = Y[Y['Outlier_Inlier']=='Inlier'].shape[0]\n",
    "        try:\n",
    "            ratio_outliers = nb_outliers_neighborhood/nb_inliers_neighborhood\n",
    "        except:\n",
    "            continue # in case of a division by zero\n",
    "    print(\"{}% outliers in neighborhood using {} neighbors\".format(int(ratio_outliers*100), n_neighbors))    \n",
    "    return X, Y, indices, distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Cross validation to find the best regularization strength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to have feature selection, we will classify the neighborhood using a lasso-regularized regression:\n",
    "\n",
    "$$\\underset{\\theta}{\\operatorname{argmin}}||Y - X\\theta||^2 + \\lambda|\\theta|$$\n",
    "\n",
    "In this section, $\\lambda$ is found using cross validation: we perform multiple logistic regressions with different regularization constant. We look at the Average Precision Recall to assess the quality of the regression. The logistic regression uses weights that will be taken into account in the loss function as shown in the equation optimization:\n",
    "\n",
    "$$\\xi(x_0) = \\underset{g \\in G}{\\operatorname{argmin}} ~\\mathcal{L} (f, g, \\pi_{x_0}) + \\Omega(g)$$\n",
    "\n",
    "As done in LIME, we use an exponential smoothing kernel to compute weights:\n",
    "\n",
    "$$K(x,y) = e^{\\frac{||x-y||^2_2}{\\sigma^2}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_rbf(x, y, sigma):\n",
    "    d = 0\n",
    "    for i in range(len(x)):\n",
    "        d_temp = (x[i]-y[i])**2 \n",
    "        d = d + d_temp\n",
    "    return np.exp(-0.5*(d/sigma**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sigma``` is the bandwidth representing the neighborhood size. LIME uses the following calculation: $bandwidth = 0.75*\\sqrt{n_{features}}$. This seems quite arbitrary, and no documentation is provided regarding this formula.\n",
    "\n",
    "It made more sense in our view to make the bandwidth dependent on the distance to the closest inlier in the neighborhood: $bandwidth = 0.05*\\sqrt{d(Inlier)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeBandwidth(labels, indices, distances):\n",
    "    for idx, indice_neighbor in enumerate(indices):\n",
    "        row = labels[labels['Outlier_Inlier'].index==indice_neighbor]\n",
    "        if(row['Outlier_Inlier'].iloc[0]=='Inlier'):\n",
    "            bandwidth = 0.05*np.sqrt(distances[idx])\n",
    "            print(\"Bandwidth={}\".format(bandwidth))\n",
    "            break\n",
    "    return bandwidth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use ```StratifiedKFold``` in order to make sure we have balanced folds, meaning that we want at least one sample from each class in every fold.\n",
    "\n",
    "We use Average Precision Recall score to compare the results. The Precision-Recall curve is better adapted than the ROC curve because"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findBestRegularization(X, Y, outlier, bandwidth):\n",
    "    n_splits = 3\n",
    "    ave_pr_dict = {}\n",
    "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True)\n",
    "    \n",
    "    for lambda_reg in np.power(10,np.arange(1,10))/(10e4):\n",
    "        \n",
    "        ave_pr_mean = 0\n",
    "        \n",
    "        for train_index, test_index in kf.split(X, Y):\n",
    "        \n",
    "            X_train, X_test = np.array(X)[train_index], np.array(X)[test_index]\n",
    "            Y_train, Y_test = np.array(Y)[train_index], np.array(Y)[test_index]\n",
    "        \n",
    "            # a) Compute weights with kernel. Inspired by LIME method https://arxiv.org/abs/1602.04938.\n",
    "            outlier_list = np.reshape(np.array(outlier),(1,-1)) * np.ones((X_train.shape[0],X_train.shape[1]))\n",
    "            sample_weight = []\n",
    "            for idx, elt in enumerate(np.array(X_train)):\n",
    "                sample_weight.append(kernel_rbf(elt, outlier_list[idx], bandwidth))\n",
    "        \n",
    "            # b) Perform logistic regression\n",
    "            clf = LogisticRegression(random_state=42, solver='liblinear', penalty='l1', C=1/lambda_reg)\n",
    "            clf.fit(X_train, np.array(Y_train).ravel(), sample_weight=sample_weight)\n",
    "        \n",
    "            proba = clf.predict_proba(X_test)\n",
    "            \n",
    "            ave_pr_mean = average_precision_score(Y_test, proba[:,1], pos_label='Outlier') + ave_pr_mean\n",
    "    \n",
    "        ave_pr_dict[lambda_reg] = ave_pr_mean / n_splits\n",
    "    \n",
    "    lambda_opt = max(ave_pr_dict, key=ave_pr_dict. get)\n",
    "    print(\"best regularization strength: {}\".format(lambda_opt))\n",
    "    print(\"Average PR: {}\".format(ave_pr_dict[lambda_opt]))\n",
    "    return lambda_opt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
